{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT_Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4tSHE5dztLW7hPclYbB2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenjitee/KenjiTee/blob/master/ViT_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjQEo1VCF6k-",
        "outputId": "48f4dd0b-aa83-42c1-f561-4e04fd95b589"
      },
      "source": [
        "! pip install transformers pytorch-lightning --quiet\n",
        "! sudo apt -qq install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 916 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 55.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 272 kB 72.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 294 kB 69.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 70.6 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tusG_DUQF-dy"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import glob\n",
        "import pytorch_lightning as pl\n",
        "from huggingface_hub import HfApi, Repository\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchmetrics import Accuracy\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LTN1BwsIJkW"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxgIvQp2JYbS"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrsAae8JdJv"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ZVYTcELYSB"
      },
      "source": [
        "!mkdir~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mZHcPL4LZf7"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86-vnwRSLa-5"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTMCujsLb8B"
      },
      "source": [
        "! kaggle datasets list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEJZ2In-LepQ"
      },
      "source": [
        "!kaggle datasets download -d grassknoted/asl-alphabet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBHUEg7LiKO"
      },
      "source": [
        "!unzip asl-alphabet.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvNye4T4LjW5"
      },
      "source": [
        "data_dir = Path(\"/content/asl_alphabet_train/asl_alphabet_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEQVnKObLkMe"
      },
      "source": [
        "ds=ImageFolder(data_dir)\n",
        "indices = torch. randperm(len(ds)).tolist()\n",
        "n_val = math.floor(len(indices) * .15)\n",
        "train_ds = torch.utils.data.Subset(ds, indices[:-n_val])\n",
        "val_ds = torch.utils.data.Subset(ds, indices[-n_val:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVXc1HKTLmY-"
      },
      "source": [
        "plt.figure(figsize=(100,50))\n",
        "num_examples_per_class = 1\n",
        "i = 1\n",
        "for class_idx, class_name in enumerate(ds.classes):\n",
        "    folder = ds.root / class_name\n",
        "    print(folder)\n",
        "    for image_idx, image_path in enumerate(sorted(folder.glob('*'))):\n",
        "        print(image_path)\n",
        "        if image_path.suffix in ds.extensions:\n",
        "            image = Image.open(image_path)\n",
        "            plt.subplot(len(ds.classes), num_examples_per_class, i)\n",
        "            ax = plt.gca()\n",
        "            ax.set_title(\n",
        "                class_name,\n",
        "                size='xx-large',\n",
        "                pad=5,\n",
        "                loc='left',\n",
        "                y=0,\n",
        "                backgroundcolor='white'\n",
        "            )\n",
        "            ax.axis('off')\n",
        "            plt.imshow(image)\n",
        "            i += 1\n",
        "\n",
        "            if image_idx + 1 == num_examples_per_class:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1x9onBpLo3Q"
      },
      "source": [
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for i, class_name in enumerate(ds.classes):\n",
        "    label2id[class_name] = str(i)\n",
        "    id2label[str(i)] = class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui7HuNspLp_m"
      },
      "source": [
        "  class ImageClassificationCollator:\n",
        "    def __init__(self, feature_extractor):\n",
        "        self.feature_extractor = feature_extractor\n",
        " \n",
        "    def __call__(self, batch):\n",
        "        encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "        return encodings "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj6ujL90LrdF"
      },
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "collator = ImageClassificationCollator(feature_extractor)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, collate_fn=collator, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, collate_fn=collator, num_workers=2)\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=len(label2id),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nfQdcTQMoZ5"
      },
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "\n",
        "\n",
        "    def __init__(self, model, lr: float = 2e-5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters('lr', *list(kwargs))\n",
        "        self.model = model\n",
        "        self.forward = self.model.forward\n",
        "        self.val_acc = Accuracy()\n",
        "        self.train_acc= Accuracy()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"train_loss\", outputs.loss)\n",
        "        acc1 = self.train_acc(outputs.logits.argmax(1), batch['labels'])\n",
        "        self.log(f\"train_acc\", acc1, prog_bar=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        self.log(f\"val_loss\", outputs.loss)\n",
        "        acc = self.val_acc(outputs.logits.argmax(1), batch['labels'])\n",
        "        self.log(f\"val_acc\", acc, prog_bar=True)\n",
        "        return outputs.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr,weight_decay=0.0025)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPyIrbzBM-ch"
      },
      "source": [
        "pl.seed_everything(42)\n",
        "classifier = Classifier(model, lr=2e-5)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='/content/trainmebby',\n",
        "    filename='ViT-{epoch:02d}-{val_loss:.2f}',\n",
        ")\n",
        "trainer = pl.Trainer(callbacks=[checkpoint_callback],gpus=1, precision=16, max_epochs=3)\n",
        "trainer.fit(classifier, train_loader, val_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOm8sCHYNDBT"
      },
      "source": [
        "test_data_path = '/content/asl_alphabet_test/asl_alphabet_test'\n",
        "image_path1= '/content/asl_alphabet_test/asl_alphabet_test/O_test.jpg'\n",
        "images_path=glob.glob(test_data_path+'/*.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWoelZsJVjsW"
      },
      "source": [
        "def prediction(img_path):\n",
        "  im=Image.open(img_path)\n",
        "  encoding = feature_extractor(images=im, return_tensors=\"pt\")\n",
        "  encoding.keys()\n",
        "\n",
        "  pixel_values = encoding['pixel_values']\n",
        "\n",
        "  outputs = model(pixel_values)\n",
        "  result = outputs.logits.softmax(1).argmax(1)\n",
        "  new_result = result.tolist()\n",
        "  for i in new_result:\n",
        "    return(id2label[str(i)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW2mzEiAMtTi"
      },
      "source": [
        " def process_image(image_path):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    pil_image = Image.open(image_path)\n",
        "    \n",
        "    # Resize\n",
        "    if pil_image.size[0] > pil_image.size[1]:\n",
        "        pil_image.thumbnail((5000, 256))\n",
        "    else:\n",
        "        pil_image.thumbnail((256, 5000))\n",
        "        \n",
        "    # Crop \n",
        "    left_margin = (pil_image.width-224)/2\n",
        "    bottom_margin = (pil_image.height-224)/2\n",
        "    right_margin = left_margin + 224\n",
        "    top_margin = bottom_margin + 224\n",
        "    \n",
        "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
        "    \n",
        "    # Normalize\n",
        "    np_image = np.array(pil_image)/255\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image - mean) / std\n",
        "    \n",
        "    # PyTorch expects the color channel to be the first dimension but it's the third dimension \n",
        "    # in the PIL image and Numpy array\n",
        "    # Color channel needs to be first; retain the order of the other two dimensions.\n",
        "    np_image = np_image.transpose((2, 0, 1))\n",
        "    \n",
        "    return np_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPrnp3RpU-gr"
      },
      "source": [
        "def imshow(image, ax=None, title=None):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1CLwF3GVFy0"
      },
      "source": [
        "def display_image(image_dir):\n",
        "\n",
        "    # Plot flower input image\n",
        "    plt.figure(figsize = (6,10))\n",
        "    plot_1 = plt.subplot(2,1,1)\n",
        "    \n",
        "    image = process_image(image_dir)\n",
        "    \n",
        "\n",
        "    asl_sign = image_dir[image_dir.rfind('/')+1:]\n",
        "    \n",
        "    pred= prediction(image_dir)\n",
        "\n",
        "    plot_1.set_xlabel(\"The predicted sign: \"+pred)\n",
        "\n",
        "    imshow(image, plot_1, title=asl_sign);\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4brq-DmXB5H"
      },
      "source": [
        "for i in images_path:\n",
        "  display_image(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VggII2dTXE1I"
      },
      "source": [
        "display_image(image_path1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
